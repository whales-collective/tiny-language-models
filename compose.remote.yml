
services:
  pizza-test:
    build: 
      context: .
      dockerfile: pizza_test.Dockerfile
    command: go test -v -run TestHawaiianPizzaExpert
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
      - MODEL_RUNNER_LLM_CHAT=${MODEL_RUNNER_LLM_CHAT}

    depends_on:
      download-llm:
        condition: service_completed_successfully

  star-trek-test:
    build: 
      context: .
      dockerfile: star_trek_test.Dockerfile
    command: go test -v -run TestStarTrekExpert
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
      - MODEL_RUNNER_LLM_CHAT=${MODEL_RUNNER_LLM_CHAT}
    depends_on:
      download-llm:
        condition: service_completed_successfully

  download-llm:
    image: curlimages/curl:8.6.0
    entrypoint: |
      sh -c '
      # Download Chat model
      curl -s "${MODEL_RUNNER_BASE_URL}/models/create" -d @- << EOF
      {"from": "${MODEL_RUNNER_LLM_CHAT}"}
      EOF
      '
  